{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec53457",
   "metadata": {},
   "source": [
    "RAG PIPELINE IMPROVEMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9489ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load your .env file containing GROQ_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c703c4",
   "metadata": {},
   "source": [
    "1] DATA INGECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, WebBaseLoader\n",
    "\n",
    "# PDF data injection\n",
    "pdf_loader = PyPDFLoader(\"DeepLearning.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Text data injection\n",
    "text_loader = TextLoader(\"MachineLearning.txt\")\n",
    "text_docs = text_loader.load()\n",
    "\n",
    "# Web data injection\n",
    "web_loader = WebBaseLoader(\"https://www.geeksforgeeks.org/artificial-intelligence/what-is-generative-ai/\")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "# Combine all\n",
    "all_docs = pdf_docs + text_docs + web_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3473ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285749a9",
   "metadata": {},
   "source": [
    "3] DATA EMBEDDING AND VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8873c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embedding_model) \n",
    "vector_store.save_local(\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83164b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fca0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch the GROQ API key from the environment\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize the LLM using the API key from .env\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "# Load FAISS index (assumes you already created faiss_index with the same embedding model)\n",
    "vector_store = FAISS.load_local(\n",
    "    \"faiss_index\", \n",
    "    embedding_model, \n",
    "    allow_dangerous_deserialization=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cea427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the context below to answer the question.\n",
    "If you don't know the answer, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c60823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Machine learning is a branch of Artificial Intelligence that focuses on developing models and algorithms that let computers learn from data without being explicitly programmed for every task. In simple words, ML teaches the systems to think and understand like humans by learning from the data.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Machine Learning?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788024a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
